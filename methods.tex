The major methodological contribution of this paper is that we demonstrated
a statistical analysis, described below, to determine a best estimate
for when two transitions occur: from less to more frequent usage of 
figurative violence, then back from this elevated state of usage to the pre-debate
level. This method also reveals transitory periods.
Our observational data was transcripts of cable news programming from six
cable news shows. This data was obtained by processing closed captioning
accessed from the Internet Archive's TV News Archive (TVNA). The TV News Archive
is an excellent, free research tool that constantly 
records TV news from across the country. Currently over 1,359,000 shows with
metadata and closed captions are accessibly through their website, 
\url{https://archive.org/tv/details}. We download each show's closed captions,
which are then converted to transcripts, 
which are in turn searched and coded for usage of
figurative violence. The usages are then counted, which gives us our most raw
version of quantitative data.

\subsection{Corpus}
\label{subsec:Corpus}

The corpus is a set of 607 transcripts from cable news shows aired from
September 1, 2016 to November 30, 2016 (inclusive). The transcripts were from
six shows from three networks, Fox News, MSNBC, and CNN, listed in  
Table \ref{tab:shows}. These shows were the two most watched shows from 
each network for the month of October, 2016 \cite{Katz2016}.  

\vspace{.2in}

\begin{table}[!htb]
  \centering
    \begin{tabular}{|c|c|}
      \hline
        Network & Programs \\
      \hline
      \multirow{2}{*}{Fox News} 
        & The O'Reilly Factor \\
        & The Kelly File \\
      \hline
      \multirow{2}{*}{MSNBC}
        & The Rachel Maddow Show \\
        & The Last Word with Lawrence O'Donnell \\
      \hline
      \multirow{2}{*}{CNN}
        & Anderson Cooper 360 \\
        & Erin Burnett OutFront \\
      \hline
    \end{tabular}
  \caption{List of shows used in corpus and their networks.}
  \label{tab:shows}
\end{table}


We accessed the closed captions of these shows from the 
Internet Archive's (\url{http://archive.org}) 
TV News Archive (TVNA) (\url{http://archive.org/tv/details}) 
The TVNA provides a stable, though hidden, application programming interface
(API) for programmatic access to the data stored there. 
The TVNA has a wealth of metadata on each show including the original air date, 
run time, links back to the TVNA, and clip download links. These are all used to 
build rich software for corpus building and annotation. 

A limitation of the TVNA is that clips and closed captions are only available
in up to 60-second chunks. 
Requesting content from a URL that requests more than this amount will result
in an error. So to build our transcripts, we must make many requests for 
60-second segments of a show's closed captions. These closed captions were then
concatenated to create a full show's closed caption transcript. The final 
transcript of the show was created using PBS's \texttt{pycation} package, which
converts SRT-formatted closed-captions to a text format where every change in
speaker is represented by a new paragraph \cite{PBS2016, Matroska2016}. 

Searching for shows, accessing each show's metadata, and downloading the 
transcipts was done with a new open-source Python package I developed 
called \textit{iatv} (\url{https://github.com/mtpain/iatv}).
This encapsulates API calls to resources such as 
\url{https://archive.org/details/MSNBCW\_20160910\_020000\_The\_Last\_Word\_With\_Lawrence\_ODonnell?output=json}, which contains JSON-formatted metadata for the show, including
links to files that are available for download. The closed-caption files are
available for download by making a request from a URL like the following
\url{https://archive.org/download/MSNBCW\_20160910\_020000\_The\_Last\_Word\_With\_Lawrence\_ODonnell.cc5.srt?t=0/60}.
The query string, \texttt{t=0/60} requests the first sixty seconds of closed
captions. These two numbers must be sequentially incremented to take the
next sixty seconds. If the show is an hour long, this would result in 60 API
calls to download every closed caption clip for that show. The shows' closed
captions, the built transcript, and other relevant metadata is stored in a
MongoDB database, which is further extended with analyst annotations as
potential metaphors are searched for and metaphors are identified for 
inclusion in our analysis.


\subsection{Metaphor identifcation}
\label{sub:metaphor-identifcation}

We identify metaphorical violence by first searching for a set of violent words.
These words were selected as a starting point. Out of the list of words we 
considered, we present results from constructions using \textit{attack}, \textit{hit},
and \textit{beat}. 
Every match is tagged and is added to a MongoDB database, along with other metadata, 
as a potential usage of metaphorical violence. Every databse entry contains 
the paragraph containing the the instance of the violent word, the program
in which it was spoken, the date, and a link back to the show on the Internet
Archive for further investigation. These potential instances are metaphorical
violence were displayed in an open-source 
web application\footnote{\url{https://github.com/mtpain/metacorps}} specially 
designed for this project. This web application shares the same data model we
use for organizing the metaphors. 

Paragraphs with a match to one of the violence words or phrases were then
displayed in the user interface of the web application. 
Each instance was examined and marked for inclusion if the 
violence was in fact figurative.
Other information was also collected, including the underlying conceptual
metaphor, who or what was the subject and
object of the figurative violence, the tense, and whether the construction was
active or passive. Instances of course were included only if the violent word
was used in a figurative way. However, this could include constructions like
``the noreaster hit the East Coast on Thursday.'' Therefore, a further condition
on inclusion was whether the figurative violence was being done by or to someone
or something involved in politics. This person did not have to be a United States
citizen. For example, one instance that was included was 
``sanctions...hit Putin as an individual in his inner circle.'' 


\subsection{Statistical Analysis}
\label{sub:statistical-analysis}

To review, the goal of the statistical analysis is to determine if and when
phase transitions occur to or from higher or lower frequency of use 
of figurative violence
during the campaign. Preliminary analyses of the data revealed that by far
the most commonly used violent words in figurative violence constructions were
\textit{attack}, \textit{hit}, and \textit{beat}. In order to have higher 
counts and increase statistical power, usages were summed across all programs,
so each day we counted the total number of uses of each one of those words.
There is a sampling of this data table in Table \ref{tab:sample-data}. In 
order to assign phases one, two, and three to each row of this table, we
must first decide what candidate dates we should use for the first and
second phase transition. Part of the analysis is to find what the optimal 
dates are in terms of relative likelihood that the model with two particular
phase transition dates captures more information than any other candidate
model, including the null hypothesis, or no dependence on phase.

% \begin{table}[ht]
%     \centering
%     \begin{tabular}{|r|cc|}
%         \hline
%         date       &   facet    &  count     \\
%         \hline
%         2016-09-01 &     attack &    0     \\
%         2016-09-02 &     hit    &    1     \\
%         2016-09-03 &     hit    &    2     \\
%         2016-09-04 &     beat   &    0     \\
%         2016-09-05 &     beat   &    1     \\
%         $\vdots$   &  $\vdots$  & $\vdots$ \\
%     \end{tabular}
%     \caption{Example first five rows of data used in statistical analysis to find
%     phase transition before addition of phase}
%     \label{tab:sample-data}
% \end{table}

% \begin{table}[ht]
%     \centering
%     \begin{tabular}{|r|ccc|}
%         \hline
%         date       &  phase  &   facet    &  count     \\
%         \hline               
%         2016-09-01 &    1    &     attack &    0     \\
%         2016-09-02 &    1    &     hit    &    1     \\
%         2016-09-03 &    1    &     hit    &    2     \\
%         2016-09-04 &    1    &     beat   &    0     \\
%         2016-09-05 &    1    &     beat   &    1     \\
%         $\vdots$   & $\vdots$&  $\vdots$  & $\vdots$ \\
%     \end{tabular}
%     \caption{Example first five rows of data used in statistical analysis to find
%     phase transition with addition of phase}
%     \label{tab:sample-data-wphase}
% \end{table}


To model the data we used general linear mixed models, 
with \texttt{phase} and \texttt{facet} 
being fixed effects. Because the data is count data, Poisson regression was
used. Because there could be fluctuations on any given day,
\texttt{date} is modeled as a random effect. Including random effects are
preferred to simply averaging as this results in less information loss
\cite{Winter2013, Burnham2011, Clark1973}. Model generation was done with
the \texttt{lme4} R package \cite{Bates2015} called seamlessly 
from Python using the \texttt{rpy2} package \cite{Gautier2017}. To summarize,
the two basic models tested are given in Table \ref{tab:models}. The 
theoretical and methodological innovation presented here is in building 
a series of 266 models with 14 candidate dates for the first phase transition
and 19 candidate dates for the second phase transition\footnote{More were
tested in preliminary stages; these correspond to a more specific region of
interest}. In this way, we are actually comparing 267 models: one is the null
hypothesis, that there is no improvement in model performance if we do not
include phase in the model. The 266 other models include phase in the model,
but the dates where we assign each phase are shifted. We performed preliminary
analysis to determine that it was significant to add \texttt{facet} as a 
fixed effect.

\begin{table}[ht]
    \centering
    \begin{tabular}{|r|c|}
        \hline
        description & formula \\
        \hline
        null model, facet only & \texttt{count $\sim$ facet + (1|date)} \\
        includes phase       & \texttt{count $\sim$ phase + facet + (1|date)} \\
        \hline
    \end{tabular}
    \caption{Two basic models considered for modeling figurative violence usage}
    \label{tab:models}
\end{table}

While one could use the chi-square test and significance values to evaluate
the improvement from one model to another, this becomes unwieldy and 
questionable when comparing 267 models. A more modern approach is to compare
the Akaike information criterion (AIC) of the models 
\cite{Akaike1974, Burnham2004, Burnham2011}. The AIC is a measure of 
information loss in a model. It has no meaning on its own, but it is instead
used to calculate the likelihood that another candidate model minimizes 
information loss better than the model with minimum AIC, or 
AIC$_{\mathrm{min}}$. Comparing model $i$ with AIC larger than 
the model with AIC$_{\mathrm{min}}$ is done by calculating the 
the relative likelihood, given by

\begin{equation}
    \mathcal{L}_i = \exp{\left(
      \frac{\mathrm{AIC}_{\mathrm{min}} - \mathrm{AIC}_{i}}{2} 
      \right)
      }
  \label{eq:relative-likelihood}
\end{equation}

\noindent
so called because it yields the probability that model $i$ is actually the one
that results in a lower information loss than the model with AIC$_{\mathrm{min}}$.


In Section \ref{sec:Results}, Figures \ref{fig:AICs} and \ref{fig:relative-likelihoods}
show graphically the calculations of the AIC for all candidate phase
models and the relative likelihood of each candidate phase model, respectively.
